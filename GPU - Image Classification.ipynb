{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This notebook trains a convolutional neural network (CNN) for image classification on a typical dataset (CIFAR-100 for now). The dataset consists of 100 labels with 600 images per label. 500 images/label are in the train set and 100 images/label are in the test set.\n",
    "\n",
    "Training can be offloaded to a GPU by choosing the appropriate value for the variable 'device' below.\n",
    "\n",
    "Since the goal is to compare run-times and pricing, we aim for consistency (fixed model architecture, dataset, hyperparameters etc.) rather than tricks to speed up learning or improve model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.models import resnet34\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(device=='cuda') #don't run this notebook if GPU not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are not using a pretrained network, we do **not** need to use these normalization constants \n",
    "#but we will do so anyway in case we want to compare the performance to a pre-trained network.\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_data():\n",
    "    '''Download dataset and apply preprocessing transforms.\n",
    "    '''\n",
    "    LOCAL_PATH = './data/CIFAR100'\n",
    "    \n",
    "    train_data = CIFAR100(LOCAL_PATH, \n",
    "                          train=True, \n",
    "                          download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.Resize((224, 224)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=MEAN, std=STD)                              \n",
    "                          ])\n",
    "                         )\n",
    "    \n",
    "    test_data = CIFAR100(LOCAL_PATH,\n",
    "                         train=False, \n",
    "                         download=True,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.Resize((224, 224)),\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(mean=MEAN, std=STD)                              \n",
    "                         ])\n",
    "                        )\n",
    "    \n",
    "    label_names = pickle.load(open(f'{LOCAL_PATH}/cifar-100-python/meta', \"rb\"), encoding='ISO-8859-1')[\"fine_label_names\"]\n",
    "    \n",
    "    return train_data, test_data, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, label_names = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_data, test_data, batch_size, pin_memory=True):\n",
    "    '''Create dataloaders that create batches for training.\n",
    "    '''\n",
    "    train_dl = DataLoader(train_data, \n",
    "                          batch_size=batch_size, \n",
    "                          pin_memory=pin_memory)\n",
    "    \n",
    "    test_dl = DataLoader(test_data, \n",
    "                         batch_size=batch_size, \n",
    "                         pin_memory=pin_memory)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = create_dataloaders(train_data, test_data, 128, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Loss Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34(pretrained=False)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    acc_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (images, labels) in enumerate(train_dl): #loop over batches\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, labels)\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of images seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_acc = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            acc_dict[epoch] = val_acc\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_acc = {val_acc}')\n",
    "            \n",
    "    return model, acc_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    N_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (images, labels) in enumerate(test_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(images)\n",
    "            preds_nonprob = preds.argmax(dim=1)\n",
    "            \n",
    "            N_correct += (labels==preds_nonprob).sum().item() #accuracy computation\n",
    "            \n",
    "            loss += criterion(preds, labels) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    return loss / N, N_correct/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter = 0 Train Loss = 0.029962537961006164 val_loss = 0.027555299922823906 val_acc = 0.1675\n",
      "Iter = 1 Train Loss = 0.029955543751716614 val_loss = 0.025803718715906143 val_acc = 0.2005\n",
      "Iter = 2 Train Loss = 0.023886292572021486 val_loss = 0.022289138287305832 val_acc = 0.29\n",
      "Iter = 3 Train Loss = 0.0202695588350296 val_loss = 0.02001195400953293 val_acc = 0.3484\n",
      "Iter = 4 Train Loss = 0.017271596004962923 val_loss = 0.017855264246463776 val_acc = 0.4118\n",
      "Iter = 5 Train Loss = 0.014594539954662323 val_loss = 0.01709928922355175 val_acc = 0.4385\n",
      "Iter = 6 Train Loss = 0.012312490348815918 val_loss = 0.01777018792927265 val_acc = 0.4387\n",
      "Iter = 7 Train Loss = 0.010540116629600525 val_loss = 0.018000656738877296 val_acc = 0.4478\n",
      "Iter = 8 Train Loss = 0.008653313562870026 val_loss = 0.01896815374493599 val_acc = 0.4467\n",
      "Iter = 9 Train Loss = 0.006887058638334274 val_loss = 0.021302219480276108 val_acc = 0.4512\n",
      "Iter = 10 Train Loss = 0.005184352113008499 val_loss = 0.022613035514950752 val_acc = 0.4604\n",
      "Iter = 11 Train Loss = 0.00378568950265646 val_loss = 0.023444436490535736 val_acc = 0.4642\n",
      "Iter = 12 Train Loss = 0.0026580386932194234 val_loss = 0.027589483186602592 val_acc = 0.4617\n",
      "Iter = 13 Train Loss = 0.0020262540094554424 val_loss = 0.026689568534493446 val_acc = 0.4758\n",
      "Iter = 14 Train Loss = 0.0014818633008003234 val_loss = 0.027188146486878395 val_acc = 0.4789\n",
      "Iter = 15 Train Loss = 0.0012025533080473542 val_loss = 0.029456032440066338 val_acc = 0.4734\n",
      "Iter = 16 Train Loss = 0.0010780763781815766 val_loss = 0.03017936274409294 val_acc = 0.4915\n",
      "Iter = 17 Train Loss = 0.0009513799727708101 val_loss = 0.031098172068595886 val_acc = 0.4571\n",
      "Iter = 18 Train Loss = 0.0008367637664824725 val_loss = 0.030910931527614594 val_acc = 0.4853\n",
      "Iter = 19 Train Loss = 0.0007583957659453153 val_loss = 0.03328949958086014 val_acc = 0.4786\n",
      "CPU times: user 1h 28min 27s, sys: 8min 42s, total: 1h 37min 9s\n",
      "Wall time: 52min 7s\n"
     ]
    }
   ],
   "source": [
    "%time model, acc_dict, loss_dict = train_model(train_dl, test_dl, model, criterion, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
